{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vMdbE8CBPYoM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgQ8EIJeOSSq",
        "outputId": "08beaee2-e26c-4951-c070-482f63b52eab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TabTransf + naive"
      ],
      "metadata": {
        "id": "TeD9bA_AOgvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VAEQ8lFpN5x7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "\n",
        "# classes\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "# attention\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            heads=8,\n",
        "            dim_head=16,\n",
        "            dropout=0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "# transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.Embedding(num_tokens, dim)     # all number of tokens to hidd dim\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=attn_dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, dropout=ff_dropout))),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeds(x)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x)\n",
        "            x = ff(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# mlp\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dims, act=None):\n",
        "        super().__init__()\n",
        "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
        "        layers = []\n",
        "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
        "            is_last = ind >= (len(dims_pairs) - 1)\n",
        "            linear = nn.Linear(dim_in, dim_out)\n",
        "            layers.append(linear)\n",
        "\n",
        "            # removed last layer to get logits\n",
        "            # if is_last:\n",
        "            #     continue\n",
        "            # act = default(act, nn.ReLU())\n",
        "            # layers.append(act)\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "# main class\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            categories,\n",
        "            num_continuous,\n",
        "            dim,\n",
        "            depth,\n",
        "            heads,\n",
        "            dim_head=16,\n",
        "            dim_out=1,\n",
        "            mlp_hidden_mults=(4, 2),\n",
        "            mlp_act=None,\n",
        "            num_special_tokens=2,\n",
        "            continuous_mean_std=None,\n",
        "            attn_dropout=0.,\n",
        "            ff_dropout=0.,\n",
        "            seed=42\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
        "\n",
        "        self.categories = categories\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # categories related calculations\n",
        "\n",
        "        self.num_categories = len(categories)  # len of input sequences\n",
        "        self.num_unique_categories = sum(categories)  # number of all categories\n",
        "\n",
        "        # create category embeddings table\n",
        "\n",
        "        self.num_special_tokens = num_special_tokens  # for missing?\n",
        "        total_tokens = self.num_unique_categories + num_special_tokens\n",
        "\n",
        "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
        "\n",
        "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value=num_special_tokens)  # ??\n",
        "        categories_offset = categories_offset.cumsum(dim=-1)[:-1]  # cumulative sum\n",
        "        # if categ = (2, 3, 2, 3), categories_offset = tensor([0, 2, 5, 7]), bias for column categs\n",
        "        self.register_buffer('categories_offset', categories_offset)  # save it and use in forward\n",
        "\n",
        "        # continuous\n",
        "\n",
        "        if exists(continuous_mean_std):\n",
        "            assert continuous_mean_std.shape == (num_continuous,\n",
        "                                                 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
        "        self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
        "\n",
        "        self.norm = nn.LayerNorm(num_continuous)\n",
        "        self.num_continuous = num_continuous\n",
        "\n",
        "        # transformer\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            num_tokens=total_tokens,\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            dim_head=dim_head,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout\n",
        "        )\n",
        "\n",
        "        # mlp to logits\n",
        "\n",
        "        input_size = (dim * self.num_categories) + num_continuous\n",
        "        l = input_size // 8\n",
        "\n",
        "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
        "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
        "\n",
        "        self.mlp = MLP(all_dimensions, act=mlp_act)\n",
        "\n",
        "    def forward(self, x_categ, x_cont):\n",
        "        assert x_categ.shape[\n",
        "                   -1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
        "        x_categ += self.categories_offset    #    TODO\n",
        "\n",
        "        x = self.transformer(x_categ)\n",
        "\n",
        "        flat_categ = x.flatten(1)\n",
        "\n",
        "        assert x_cont.shape[\n",
        "                   1] == self.num_continuous, f'you must pass in {self.num_continuous} values for your continuous input'\n",
        "\n",
        "        if exists(self.continuous_mean_std):\n",
        "            mean, std = self.continuous_mean_std.unbind(dim=-1)  # splits tensor into 2 parts\n",
        "            x_cont = (x_cont - mean) / std\n",
        "\n",
        "        normed_cont = self.norm(x_cont)  # norm over all batch\n",
        "\n",
        "        x = torch.cat((flat_categ, normed_cont), dim=-1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def naive_recover(self, data_nan, nans_pos, device):\n",
        "        # now x_categ contains NaNs in its rows\n",
        "        # our task is to find the most suitable token in the corresponding column tokens\n",
        "        self.eval()\n",
        "        recovered_labels = []\n",
        "        for j in tqdm(range(len(nans_pos))):\n",
        "            i = nans_pos[j]\n",
        "            # create tensor with many variants for substitution\n",
        "            row = np.array([data_nan[j]] * self.categories[i])\n",
        "            row[:, i] = np.arange(self.categories[i])\n",
        "            # get contextual embeddings\n",
        "            x_categ = (torch.LongTensor(row).to(device) + self.categories_offset)    # (N, input)\n",
        "            with torch.no_grad():\n",
        "                w_categ = self.transformer(x_categ)    # (N, input, hidden)\n",
        "            # choose the best\n",
        "            v = w_categ[:, i]\n",
        "            w_categ = torch.cat((w_categ[:, :i], w_categ[:, i+1:]), dim=1)\n",
        "            w_pairs = torch.matmul(w_categ, v.unsqueeze(2)).squeeze(2)    # (N,imp,h)x(N,h,1)=(N,inp,1)->(N,inp)\n",
        "            losses = -nn.LogSoftmax(dim=1)(w_pairs).sum(dim=1)    # (N)\n",
        "            recovered_labels.append(torch.argmin(losses).detach().cpu().item())\n",
        "\n",
        "        return np.array(recovered_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_simple_tab"
      ],
      "metadata": {
        "id": "lyav1ie5Outp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def my_train_test_split(X1, X2, y, test_size=0.2, val_size=0.05, random_state=42):\n",
        "    # X1 - categorical, X2 - continious, y - labels\n",
        "    assert X1.shape[0] == X2.shape[0] == y.shape[0]\n",
        "    n1 = int(np.around(test_size * y.shape[0]))\n",
        "    n2 = int(np.around((val_size + test_size) * y.shape[0]))\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    idx = np.random.permutation(y.shape[0])\n",
        "\n",
        "    X1_test = X1[idx][:n1]\n",
        "    X2_test = X2[idx][:n1]\n",
        "    y_test = y[idx][:n1]\n",
        "\n",
        "    X1_val = X1[idx][n1:n2]\n",
        "    X2_val = X2[idx][n1:n2]\n",
        "    y_val = y[idx][n1:n2]\n",
        "\n",
        "    X1_train = X1[idx][n2:]\n",
        "    X2_train = X2[idx][n2:]\n",
        "    y_train = y[idx][n2:]\n",
        "\n",
        "    return X1_train, X2_train, y_train, X1_val, X2_val, y_val, X1_test, X2_test, y_test\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def count_acc(y_true, y_pred):\n",
        "    # x = (torch.round(nn.Sigmoid()(y_pred)) == y_true).sum().float().item() / y_true.shape[0] / y_true.shape[1]\n",
        "    # y = accuracy_score(torch.round(nn.Sigmoid()(y_pred)).detach().numpy(), y_true.detach().numpy())\n",
        "    # assert x == y\n",
        "    return (torch.round(nn.Sigmoid()(y_pred)) == y_true).sum().float().item() / y_true.shape[0]\n",
        "    # return accuracy_score(torch.round(nn.Sigmoid()(y_pred)).detach().numpy(), y_true.detach().numpy())\n",
        "    # return x\n",
        "\n",
        "\n",
        "def count_auc(model, device, *dataloaders):\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for dataloader in dataloaders:\n",
        "            for i, (x_categ, x_cont, labels) in enumerate(dataloader):\n",
        "                x_categ, x_cont, labels = x_categ.long().to(device), x_cont.float().to(device), labels.float().to(\n",
        "                    device)\n",
        "                pred = nn.Sigmoid()(model.forward(x_categ, x_cont))\n",
        "                y_true.extend(list(labels.detach().cpu().numpy()))\n",
        "                y_pred.extend(list(pred.detach().cpu().numpy()))\n",
        "\n",
        "    return roc_auc_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def count_pres_rec_f1(model, device, *dataloaders):\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for dataloader in dataloaders:\n",
        "            for i, (x_categ, x_cont, labels) in enumerate(dataloader):\n",
        "                x_categ, x_cont, labels = x_categ.long().to(device), x_cont.float().to(device), labels.float().to(\n",
        "                    device)\n",
        "                pred = nn.Sigmoid()(model.forward(x_categ, x_cont))\n",
        "                y_true.extend(list(labels.detach().cpu().numpy()))\n",
        "                y_pred.extend(list(torch.round(pred).detach().cpu().numpy()))\n",
        "\n",
        "    return precision_score(y_true, y_pred, zero_division=0), \\\n",
        "           recall_score(y_true, y_pred, zero_division=0), \\\n",
        "           f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "\n",
        "def single_pass(model, dataloader, loss_func, device, optim=None):\n",
        "    loss_count, acc_count = 0, 0\n",
        "    for i, (x_categ, x_cont, labels) in enumerate(dataloader):\n",
        "        x_categ, x_cont, labels = x_categ.long().to(device), x_cont.float().to(device), labels.float().to(device)\n",
        "        pred = model.forward(x_categ, x_cont)\n",
        "        loss = loss_func(pred, labels)\n",
        "        loss_count += loss.item()\n",
        "        acc_count += count_acc(labels, pred)\n",
        "        # roc_auc += roc_auc_score(labels, pred, average='macro')\n",
        "        if optim is not None:\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "    return loss_count / len(dataloader), acc_count / len(dataloader)\n",
        "\n",
        "\n",
        "def plot_results(ax, train_results: list, val_results: list, test_result, label):\n",
        "    epochs = np.arange(1, len(train_results) + 1)\n",
        "    ax.plot(epochs, train_results, label='train')\n",
        "    ax.plot(epochs, val_results, label='validation')\n",
        "    ax.plot(epochs[-1], test_result,\n",
        "            marker='o', linestyle='none', label='test')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel(label)\n",
        "    ax.grid(linestyle=':')\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "def train_model(model, loss, optim, epochs, device, dataloaders, single_pass=single_pass):\n",
        "    dataloader_train, dataloader_val, dataloader_test = dataloaders\n",
        "    train_loss_all, val_loss_all, train_acc_all, val_acc_all = [], [], [], []\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        # train\n",
        "        # print('train')\n",
        "        train_loss, train_acc = single_pass(model, dataloader_train, loss, device, optim)\n",
        "        # print('val')\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_acc = single_pass(model, dataloader_val, loss, device)\n",
        "\n",
        "        print(\n",
        "            f'epoch {epoch}, train_loss={train_loss}, validation_loss={val_loss}, train_acc={train_acc}, val_acc={val_acc}')\n",
        "\n",
        "        train_loss_all.append(train_loss)\n",
        "        val_loss_all.append(val_loss)\n",
        "        train_acc_all.append(train_acc)\n",
        "        val_acc_all.append(val_acc)\n",
        "\n",
        "    # test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss, test_acc = single_pass(model, dataloader_test, loss, device)\n",
        "        test_AUC = count_auc(model, device, dataloader_test)\n",
        "        pres, rec, f1 = count_pres_rec_f1(model, device, dataloader_test)\n",
        "\n",
        "        print(f'test_loss={test_loss}, test_acc={test_acc}')\n",
        "        print('test_AUC=', test_AUC)\n",
        "        print('pres=', pres, 'rec=', rec, 'f1=', f1)\n",
        "    model.train()\n",
        "    # print(count_auc(model, device, dataloader_train, dataloader_val, dataloader_test))\n",
        "    return train_loss_all, val_loss_all, train_acc_all, val_acc_all, test_loss, test_acc\n",
        "\n",
        "\n",
        "def my_subplots(train_loss, val_loss, train_acc, val_acc, test_loss, test_acc):\n",
        "    _, ax1 = plt.subplots()\n",
        "    plot_results(ax1, train_loss, val_loss, test_loss, 'Loss')\n",
        "    _, ax2 = plt.subplots()\n",
        "    plot_results(ax2, train_acc, val_acc, test_acc, 'Accuracy')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "reRz4p1eOOqH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tabtrain"
      ],
      "metadata": {
        "id": "vMdbE8CBPYoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tab_train(filename, mode='mlm_single', seed=42):  # mlm_single / mlm_different\n",
        "    if mode == 'ordinary':\n",
        "        path = os.path.join('data/preprocessed', filename)\n",
        "    elif mode == 'naive':\n",
        "        path = os.path.join('data/recovered', f'{filename}_naive')\n",
        "    elif mode == 'mlm_single':\n",
        "        path = os.path.join('data/recovered', f'{filename}_mlm_single')\n",
        "    elif mode == 'mlm_different':\n",
        "        path = os.path.join('data/recovered', f'{filename}_mlm_different')\n",
        "\n",
        "    data_categ = pd.read_csv(os.path.join(path, 'categ.csv')).to_numpy()\n",
        "    data_cont = pd.read_csv(os.path.join(path, 'cont.csv')).to_numpy()\n",
        "    data_labels = pd.read_csv(os.path.join(path, 'labels.csv')).to_numpy()\n",
        "    # print(data_cont.shape)\n",
        "\n",
        "    X1_train, X2_train, y_train, X1_val, X2_val, y_val, X1_test, X2_test, y_test = my_train_test_split(data_categ,\n",
        "                                                                                                       data_cont,\n",
        "                                                                                                       data_labels,\n",
        "                                                                                                       test_size=0.2,\n",
        "                                                                                                       val_size=0.05,\n",
        "                                                                                                       random_state=seed)\n",
        "\n",
        "    cont_mean_std = np.array([X2_train.mean(axis=0), X2_train.std(axis=0)]).transpose(1, 0)\n",
        "    cont_mean_std = torch.Tensor(cont_mean_std)\n",
        "\n",
        "    categories = tuple(len(np.unique(data_categ[:, i])) for i in range(data_categ.shape[1]))\n",
        "    # print(categories)\n",
        "    model = TabTransformer(\n",
        "        categories=categories,  # tuple containing the number of unique values within each category\n",
        "        num_continuous=data_cont.shape[-1],  # number of continuous values\n",
        "        dim=32,  # dimension, paper set at 32\n",
        "        dim_out=1,  # binary prediction, but could be anything\n",
        "        depth=6,  # depth, paper recommended 6\n",
        "        heads=8,  # heads, paper recommends 8\n",
        "        attn_dropout=0.1,  # post-attention dropout\n",
        "        ff_dropout=0.1,  # feed forward dropout\n",
        "        mlp_hidden_mults=(4, 2),  # relative multiples of each hidden dimension of the last mlp to logits\n",
        "        mlp_act=None,  # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
        "        continuous_mean_std=cont_mean_std,  # (optional) - normalize the continuous values before layer norm\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    batch_size = 1900\n",
        "    epochs = 10\n",
        "    lr = 1e-4\n",
        "    optim = Adam(model.parameters(), lr=lr)\n",
        "    loss = F.binary_cross_entropy_with_logits\n",
        "\n",
        "    dataset_train = MyDataset(X1_train, X2_train, y_train)\n",
        "    dataset_val = MyDataset(X1_val, X2_val, y_val)\n",
        "    dataset_test = MyDataset(X1_test, X2_test, y_test)\n",
        "\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=64)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=64)\n",
        "    dataloaders = [dataloader_train, dataloader_val, dataloader_test]\n",
        "\n",
        "    train_loss_all, val_loss_all, train_acc_all, val_acc_all, test_loss, test_acc = \\\n",
        "        train_model(model, loss, optim, epochs, device, dataloaders)\n",
        "\n",
        "    return train_loss_all, val_loss_all, train_acc_all, val_acc_all, test_loss, test_acc"
      ],
      "metadata": {
        "id": "sqmloR6oPYEN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# naive recover"
      ],
      "metadata": {
        "id": "C6B-ijiUPDQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def train(model, data_categ, data_cont, data_labels, seed=42):\n",
        "\n",
        "    X1_train, X2_train, y_train, X1_val, X2_val, y_val, X1_test, X2_test, y_test = my_train_test_split(data_categ,\n",
        "                                                                                                       data_cont,\n",
        "                                                                                                       data_labels,\n",
        "                                                                                                       test_size=0.2,\n",
        "                                                                                                       val_size=0.05,\n",
        "                                                                                                       random_state=seed)\n",
        "\n",
        "    continuous_mean_std = np.array([X2_train.mean(axis=0), X2_train.std(axis=0)]).transpose(1, 0)\n",
        "    continuous_mean_std = torch.Tensor(continuous_mean_std)\n",
        "    model.register_buffer('continuous_mean_std', continuous_mean_std)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    batch_size = 10000\n",
        "    epochs = 20  # 10\n",
        "    lr = 1e-4\n",
        "    optim = Adam(model.parameters(), lr=lr)\n",
        "    loss = F.binary_cross_entropy_with_logits\n",
        "\n",
        "    dataset_train = MyDataset(X1_train, X2_train, y_train)\n",
        "    dataset_val = MyDataset(X1_val, X2_val, y_val)\n",
        "    dataset_test = MyDataset(X1_test, X2_test, y_test)\n",
        "\n",
        "    # def seed_worker(worker_id):\n",
        "    #     worker_seed = torch.initial_seed() % 2 ** 32\n",
        "    #     np.random.seed(worker_seed)\n",
        "    #     random.seed(worker_seed)\n",
        "    #\n",
        "    # g = torch.Generator()\n",
        "    # g.manual_seed(0)\n",
        "\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=64)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=64)\n",
        "    dataloaders = [dataloader_train, dataloader_val, dataloader_test]\n",
        "\n",
        "    train_loss_all, val_loss_all, train_acc_all, val_acc_all, test_loss, test_acc =\\\n",
        "        train_model(model, loss, optim, epochs, device, dataloaders)\n",
        "    return train_loss_all, val_loss_all, train_acc_all, val_acc_all, test_loss, test_acc\n",
        "\n",
        "\n",
        "def separate_nans(data_categ, data_cont, data_labels):\n",
        "    # find rows with NaNs and separate them from pure data\n",
        "    # Nans rows are used to recover data in the missings\n",
        "    idx = np.array([i for i, x in enumerate(data_categ) if not any(np.isnan(x))])\n",
        "    idx = np.in1d(np.arange(data_categ.shape[0]), idx)    # idx for clear rows\n",
        "    categ_clear = data_categ[idx]\n",
        "    cont_clear = data_cont[idx]\n",
        "    labels_clear = data_labels[idx]\n",
        "\n",
        "    nidx = np.logical_not(idx)\n",
        "    categ_nan = data_categ[nidx]\n",
        "    cont_nan = data_cont[nidx]\n",
        "    labels_nan = data_labels[np.logical_not(idx)]\n",
        "    nans_pos = np.where(np.isnan(categ_nan))\n",
        "\n",
        "    # we suppose that we have only one Nan in rows\n",
        "    return categ_clear, categ_nan, cont_clear, cont_nan, labels_clear, labels_nan, nans_pos\n",
        "\n",
        "\n",
        "def main(filename, seed=42):\n",
        "    # filename = sys.argv[1]\n",
        "    # data_path = '/content/drive/MyDrive/HSE/NIR/data'\n",
        "    path = os.path.join('/content/drive/MyDrive/HSE/NIR/data/preprocessed', filename)\n",
        "    path_nans = os.path.join('/content/drive/MyDrive/HSE/NIR/data/with_nans', filename)\n",
        "    data_categ_all = pd.read_csv(os.path.join(path_nans, 'categ.csv')).to_numpy()    # read with nans file\n",
        "    true_labels = pd.read_csv(os.path.join(path_nans, 'true_labels.csv')).to_numpy().reshape(-1)\n",
        "    data_cont = pd.read_csv(os.path.join(path, 'cont.csv')).to_numpy()\n",
        "    data_labels = pd.read_csv(os.path.join(path, 'labels.csv')).to_numpy()\n",
        "\n",
        "    categ_clear, categ_nan, cont_clear, cont_nan, labels_clear, labels_nan, nans_pos =\\\n",
        "        separate_nans(data_categ_all, data_cont, data_labels)    # data_categ is pure of NaNs\n",
        "\n",
        "    categories = tuple(len(np.unique(categ_clear[:, i])) for i in range(categ_clear.shape[1]))\n",
        "\n",
        "    model = TabTransformer(\n",
        "        categories=categories,  # tuple containing the number of unique values within each category\n",
        "        num_continuous=data_cont.shape[-1],  # number of continuous values\n",
        "        dim=32,  # dimension, paper set at 32\n",
        "        dim_out=1,  # binary prediction, but could be anything\n",
        "        depth=6,  # depth, paper recommended 6\n",
        "        heads=2,  # heads, paper recommends 8\n",
        "        attn_dropout=0.1,  # post-attention dropout\n",
        "        ff_dropout=0.1,  # feed forward dropout\n",
        "        mlp_hidden_mults=(4, 2),  # relative multiples of each hidden dimension of the last mlp to logits\n",
        "        mlp_act=None,  # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
        "        continuous_mean_std=None,  # (optional) - normalize the continuous values before layer norm\n",
        "        seed=seed\n",
        "    )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    train(model, categ_clear, cont_clear, labels_clear)\n",
        "    # recover labels\n",
        "    recovered_labels = model.naive_recover(categ_nan, nans_pos[1], device)\n",
        "    print('acc', accuracy_score(true_labels, recovered_labels))\n",
        "\n",
        "    # insert them into dataset\n",
        "    categ_nan[nans_pos] = recovered_labels\n",
        "    new_categ = np.vstack((categ_clear, categ_nan))\n",
        "    new_cont = np.vstack((cont_clear, cont_nan))\n",
        "    new_labels = np.vstack((labels_clear, labels_nan))\n",
        "\n",
        "    pd.DataFrame(data=new_categ).to_csv(f'/content/drive/MyDrive/HSE/NIR/data/recovered/{filename}_naive/categ.csv', index=False)\n",
        "    pd.DataFrame(data=new_cont).to_csv(f'/content/drive/MyDrive/HSE/NIR/data/recovered/{filename}_naive/cont.csv', index=False)\n",
        "    pd.DataFrame(data=new_labels).to_csv(f'/content/drive/MyDrive/HSE/NIR/data/recovered/{filename}_naive/labels.csv', index=False)\n",
        "\n",
        "    return accuracy_score(true_labels, recovered_labels)\n"
      ],
      "metadata": {
        "id": "0dxVfauyOprh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# adult 15%"
      ],
      "metadata": {
        "id": "HRGJR_aOTT0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = []"
      ],
      "metadata": {
        "id": "gxEB_13ul5_6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.append(main(filename='adult', seed=42))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DZcFpgCPqrY",
        "outputId": "7d753a3d-26bb-4fc6-b8c7-4409bdb8b905"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train_loss=0.5990528166294098, validation_loss=0.5533750635204893, train_acc=0.7541479991204925, val_acc=0.7629193722943722\n",
            "epoch 1, train_loss=0.5404990911483765, validation_loss=0.526293312961405, train_acc=0.7627979991204925, val_acc=0.7629193722943722\n",
            "epoch 2, train_loss=0.5253934413194656, validation_loss=0.5218877539490209, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 3, train_loss=0.5201897025108337, validation_loss=0.506365446430264, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 4, train_loss=0.4923008158802986, validation_loss=0.46003398208907154, train_acc=0.7628729991204926, val_acc=0.7643398268398268\n",
            "epoch 5, train_loss=0.44238707423210144, validation_loss=0.41306940443588025, train_acc=0.7694410290237467, val_acc=0.7897050865800866\n",
            "epoch 6, train_loss=0.4075961485505104, validation_loss=0.41064611438548926, train_acc=0.8062182717678101, val_acc=0.8173024891774892\n",
            "epoch 7, train_loss=0.42802419513463974, validation_loss=0.474029310724952, train_acc=0.7841344766930519, val_acc=0.7336985930735931\n",
            "epoch 8, train_loss=0.5040497109293938, validation_loss=0.5579485793908437, train_acc=0.7302144019349165, val_acc=0.7155708874458874\n",
            "epoch 9, train_loss=0.5675783753395081, validation_loss=0.5739795556574157, train_acc=0.7212451407211962, val_acc=0.7165178571428571\n",
            "epoch 10, train_loss=0.5483934581279755, validation_loss=0.5062645222201492, train_acc=0.7251240325417766, val_acc=0.7245670995670995\n",
            "epoch 11, train_loss=0.465810127556324, validation_loss=0.41393781385638495, train_acc=0.7460781222515392, val_acc=0.7752299783549783\n",
            "epoch 12, train_loss=0.38703131675720215, validation_loss=0.36426844921979035, train_acc=0.8001035180299033, val_acc=0.830560064935065\n",
            "epoch 13, train_loss=0.36838066577911377, validation_loss=0.393538570765293, train_acc=0.825784256816183, val_acc=0.8218344155844156\n",
            "epoch 14, train_loss=0.4281052201986313, validation_loss=0.49028488496939343, train_acc=0.8075231970096746, val_acc=0.7997835497835498\n",
            "epoch 15, train_loss=0.5404016450047493, validation_loss=0.6088375295653488, train_acc=0.789284674582234, val_acc=0.7809117965367965\n",
            "epoch 16, train_loss=0.6505870670080185, validation_loss=0.6862927964239409, train_acc=0.77802627528584, val_acc=0.7766504329004329\n",
            "epoch 17, train_loss=0.7004234790802002, validation_loss=0.680694498799064, train_acc=0.7753563984168865, val_acc=0.7771239177489178\n",
            "epoch 18, train_loss=0.6626212894916534, validation_loss=0.5916286291498126, train_acc=0.7792801671064205, val_acc=0.7832792207792207\n",
            "epoch 19, train_loss=0.5543600991368294, validation_loss=0.46613073800549365, train_acc=0.7893231970096746, val_acc=0.8097267316017316\n",
            "test_loss=0.4954931758917295, test_acc=0.7915456219312602\n",
            "test_AUC= 0.8798414955605429\n",
            "pres= 0.8574938574938575 rec= 0.17268678871845622 f1= 0.28747940691927515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7326/7326 [00:29<00:00, 245.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.12653562653562653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.append(main(filename='adult', seed=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ1KIILDmBZ4",
        "outputId": "3dad7e7a-dffd-47f3-8d67-c0fdf1b35974"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train_loss=0.6094576418399811, validation_loss=0.545303304087032, train_acc=0.7432326297273527, val_acc=0.7633928571428571\n",
            "epoch 1, train_loss=0.5277517586946487, validation_loss=0.5038165359786062, train_acc=0.7628729991204926, val_acc=0.7629193722943722\n",
            "epoch 2, train_loss=0.507867231965065, validation_loss=0.5103930921265574, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 3, train_loss=0.5228726416826248, validation_loss=0.5265562651735364, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 4, train_loss=0.5314897298812866, validation_loss=0.5137490783676957, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 5, train_loss=0.502691738307476, validation_loss=0.4600656475081588, train_acc=0.7628979991204925, val_acc=0.763866341991342\n",
            "epoch 6, train_loss=0.44276632368564606, validation_loss=0.3995020732735143, train_acc=0.769785290237467, val_acc=0.7938311688311689\n",
            "epoch 7, train_loss=0.3932075798511505, validation_loss=0.3810130809292649, train_acc=0.8070740105540897, val_acc=0.8329274891774892\n",
            "epoch 8, train_loss=0.4004899635910988, validation_loss=0.43817546421831305, train_acc=0.8084397977132805, val_acc=0.7635281385281386\n",
            "epoch 9, train_loss=0.4803733602166176, validation_loss=0.5497547794472087, train_acc=0.7404637862796835, val_acc=0.7207792207792207\n",
            "epoch 10, train_loss=0.5886368900537491, validation_loss=0.6352150386030023, train_acc=0.7154810026385225, val_acc=0.7051542207792207\n",
            "epoch 11, train_loss=0.6365221589803696, validation_loss=0.6190305162559856, train_acc=0.7079707563764293, val_acc=0.7098890692640693\n",
            "epoch 12, train_loss=0.5857101380825043, validation_loss=0.5214242483630325, train_acc=0.7180041556728232, val_acc=0.7278814935064934\n",
            "epoch 13, train_loss=0.4791198894381523, validation_loss=0.41235635994058667, train_acc=0.7477871372031661, val_acc=0.7818587662337662\n",
            "epoch 14, train_loss=0.3866819739341736, validation_loss=0.3536828566681255, train_acc=0.8071125329815304, val_acc=0.834077380952381\n",
            "epoch 15, train_loss=0.3561412915587425, validation_loss=0.3675885182438475, train_acc=0.8310195030782761, val_acc=0.831642316017316\n",
            "epoch 16, train_loss=0.39823320508003235, validation_loss=0.44296987851460773, train_acc=0.814777704485488, val_acc=0.8059388528138528\n",
            "epoch 17, train_loss=0.49000826478004456, validation_loss=0.5442306516748486, train_acc=0.7871807827616535, val_acc=0.783887987012987\n",
            "epoch 18, train_loss=0.5898327082395554, validation_loss=0.6273573898907864, train_acc=0.7734223834652595, val_acc=0.7723890692640693\n",
            "epoch 19, train_loss=0.653101459145546, validation_loss=0.6526674881125941, train_acc=0.768852506596306, val_acc=0.7690746753246753\n",
            "test_loss=0.6961516404381165, test_acc=0.7624156096563012\n",
            "test_AUC= 0.868677083869923\n",
            "pres= 1.0 rec= 0.024245423057892134 f1= 0.04734299516908213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7326/7326 [00:30<00:00, 241.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.07835107835107835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.append(main(filename='adult', seed=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG7EkUj_mDkg",
        "outputId": "a7b79f9d-484e-4ffe-88ce-d004604a5753"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train_loss=0.669890746474266, validation_loss=0.5954241030143969, train_acc=0.598668491644679, val_acc=0.763866341991342\n",
            "epoch 1, train_loss=0.5691679567098618, validation_loss=0.5320686706990907, train_acc=0.7629479991204926, val_acc=0.7629193722943722\n",
            "epoch 2, train_loss=0.5262462794780731, validation_loss=0.5199894823811271, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 3, train_loss=0.5291000157594681, validation_loss=0.5406700526223038, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 4, train_loss=0.553960993885994, validation_loss=0.5636178715662523, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 5, train_loss=0.5684778839349747, validation_loss=0.5567064691673625, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 6, train_loss=0.5473040044307709, validation_loss=0.5126947625116869, train_acc=0.7628979991204925, val_acc=0.7633928571428571\n",
            "epoch 7, train_loss=0.4921822100877762, validation_loss=0.44721021796717786, train_acc=0.7634979991204924, val_acc=0.7690746753246753\n",
            "epoch 8, train_loss=0.4296054169535637, validation_loss=0.3987973097598914, train_acc=0.7775693051890941, val_acc=0.8089150432900433\n",
            "epoch 9, train_loss=0.39631537348032, validation_loss=0.4000421663125356, train_acc=0.8129785180299033, val_acc=0.8201433982683982\n",
            "epoch 10, train_loss=0.41854923218488693, validation_loss=0.4613222326293136, train_acc=0.7938076297273526, val_acc=0.743641774891775\n",
            "epoch 11, train_loss=0.49491775780916214, validation_loss=0.5568395545988372, train_acc=0.7366240325417766, val_acc=0.7061011904761905\n",
            "epoch 12, train_loss=0.5845544934272766, validation_loss=0.6262131306258115, train_acc=0.7082726033421284, val_acc=0.6900027056277056\n",
            "epoch 13, train_loss=0.6267298460006714, validation_loss=0.62087041591153, train_acc=0.6970680958663148, val_acc=0.6933170995670995\n",
            "epoch 14, train_loss=0.5918969660997391, validation_loss=0.5450574737606626, train_acc=0.7097848944591029, val_acc=0.7150974025974026\n",
            "epoch 15, train_loss=0.5029729828238487, validation_loss=0.44694656223961804, train_acc=0.734810708003518, val_acc=0.7578463203463204\n",
            "epoch 16, train_loss=0.4135245084762573, validation_loss=0.37579276977163373, train_acc=0.7870047493403695, val_acc=0.8182494588744589\n",
            "epoch 17, train_loss=0.36479468643665314, validation_loss=0.36106046266628034, train_acc=0.8243580255057168, val_acc=0.8325892857142857\n",
            "epoch 18, train_loss=0.37489937990903854, validation_loss=0.40134443884546106, train_acc=0.8209053649956024, val_acc=0.8164908008658008\n",
            "epoch 19, train_loss=0.43486273288726807, validation_loss=0.479149193926291, train_acc=0.7996738126649077, val_acc=0.7934929653679654\n",
            "test_loss=0.50965901441299, test_acc=0.780127352700491\n",
            "test_AUC= 0.8708105642110908\n",
            "pres= 0.892 rec= 0.11034141514101929 f1= 0.19638925583443415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7326/7326 [00:30<00:00, 241.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.09391209391209392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.append(main(filename='adult', seed=1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEqvXsTpmEia",
        "outputId": "e0230b32-c1b9-48da-bc51-6b4e586b0f73"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train_loss=0.6584912240505219, validation_loss=0.5740347360119675, train_acc=0.6258678759894459, val_acc=0.7614989177489178\n",
            "epoch 1, train_loss=0.5456288456916809, validation_loss=0.5131699325460376, train_acc=0.7627479991204924, val_acc=0.7629193722943722\n",
            "epoch 2, train_loss=0.5120210498571396, validation_loss=0.5175526458205599, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 3, train_loss=0.5323549062013626, validation_loss=0.5534839097297553, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 4, train_loss=0.5669949948787689, validation_loss=0.572838997299021, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 5, train_loss=0.5714035332202911, validation_loss=0.5467228627566135, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 6, train_loss=0.5276701152324677, validation_loss=0.47955697052406543, train_acc=0.7628229991204925, val_acc=0.7643398268398268\n",
            "epoch 7, train_loss=0.4566556587815285, validation_loss=0.41257644602746674, train_acc=0.7679211521547933, val_acc=0.7868641774891775\n",
            "epoch 8, train_loss=0.402444526553154, validation_loss=0.39204901592297986, train_acc=0.7990496262093227, val_acc=0.8130411255411255\n",
            "epoch 9, train_loss=0.40611811727285385, validation_loss=0.44191141652338434, train_acc=0.7959166446789798, val_acc=0.7568993506493507\n",
            "epoch 10, train_loss=0.4794372096657753, validation_loss=0.5485955539977911, train_acc=0.7346002638522428, val_acc=0.7188852813852814\n",
            "epoch 11, train_loss=0.5890216678380966, validation_loss=0.6525452146024415, train_acc=0.7153912489006157, val_acc=0.7023133116883117\n",
            "epoch 12, train_loss=0.6682395786046982, validation_loss=0.6829003294308981, train_acc=0.7013476033421284, val_acc=0.6961580086580086\n",
            "epoch 13, train_loss=0.6618798077106476, validation_loss=0.6212826271851858, train_acc=0.7021469876868954, val_acc=0.7089420995670995\n",
            "epoch 14, train_loss=0.5767851918935776, validation_loss=0.5095251870877815, train_acc=0.7142252638522427, val_acc=0.7207792207792207\n",
            "epoch 15, train_loss=0.4667155295610428, validation_loss=0.4113620600917123, train_acc=0.7457582453825857, val_acc=0.7909902597402598\n",
            "epoch 16, train_loss=0.3899412080645561, validation_loss=0.37117084048011084, train_acc=0.8113613016710641, val_acc=0.8310335497835498\n",
            "epoch 17, train_loss=0.3785073980689049, validation_loss=0.3994520369804267, train_acc=0.8178662269129288, val_acc=0.8040449134199134\n",
            "epoch 18, train_loss=0.4303641766309738, validation_loss=0.47734800522977655, train_acc=0.782851890941073, val_acc=0.7709686147186147\n",
            "epoch 19, train_loss=0.5187933295965195, validation_loss=0.5704992440613833, train_acc=0.7658979991204924, val_acc=0.7633928571428571\n",
            "test_loss=0.6031226591422008, test_acc=0.7571271481178397\n",
            "test_AUC= 0.8567132028693939\n",
            "pres= 1.0 rec= 0.002474022761009401 f1= 0.004935834155972359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7326/7326 [00:30<00:00, 237.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.10128310128310128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.append(main(filename='adult', seed=10000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkTK4DLxmFvX",
        "outputId": "0bc2517b-042e-42ce-a79e-4fd78c06f241"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train_loss=0.6139593422412872, validation_loss=0.5563425710707, train_acc=0.7574928759894459, val_acc=0.7629193722943722\n",
            "epoch 1, train_loss=0.5374782979488373, validation_loss=0.5146578333594582, train_acc=0.7628479991204926, val_acc=0.7629193722943722\n",
            "epoch 2, train_loss=0.5152581632137299, validation_loss=0.5172929501894749, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 3, train_loss=0.5284291654825211, validation_loss=0.5369074218200914, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 4, train_loss=0.5431093573570251, validation_loss=0.5346482983141234, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 5, train_loss=0.5259758979082108, validation_loss=0.49221891616330005, train_acc=0.7628229991204925, val_acc=0.7629193722943722\n",
            "epoch 6, train_loss=0.4739871248602867, validation_loss=0.4326152909885753, train_acc=0.7628479991204924, val_acc=0.7695481601731602\n",
            "epoch 7, train_loss=0.41888685524463654, validation_loss=0.40033834721102857, train_acc=0.7818975813544415, val_acc=0.8194669913419914\n",
            "epoch 8, train_loss=0.40699172765016556, validation_loss=0.4340303603446845, train_acc=0.8148752418645558, val_acc=0.7734713203463204\n",
            "epoch 9, train_loss=0.46611838042736053, validation_loss=0.5359198541352244, train_acc=0.7464734168865436, val_acc=0.7165178571428571\n",
            "epoch 10, train_loss=0.5711193233728409, validation_loss=0.6321576508608732, train_acc=0.7156963720316623, val_acc=0.7027867965367965\n",
            "epoch 11, train_loss=0.6335262656211853, validation_loss=0.6324084509502758, train_acc=0.7000918645558487, val_acc=0.7051542207792207\n",
            "epoch 12, train_loss=0.5948059409856796, validation_loss=0.5408650302525723, train_acc=0.7108945250659631, val_acc=0.7221996753246753\n",
            "epoch 13, train_loss=0.4920724779367447, validation_loss=0.43184242284659186, train_acc=0.7352408311345647, val_acc=0.762107683982684\n",
            "epoch 14, train_loss=0.40048860758543015, validation_loss=0.3684687637018435, train_acc=0.7959221635883904, val_acc=0.8286661255411255\n",
            "epoch 15, train_loss=0.36711613833904266, validation_loss=0.37451441902102844, train_acc=0.8247656112576957, val_acc=0.8218344155844156\n",
            "epoch 16, train_loss=0.3992404639720917, validation_loss=0.4352505089658679, train_acc=0.808977088830255, val_acc=0.8012040043290043\n",
            "epoch 17, train_loss=0.4741206616163254, validation_loss=0.5180905072978048, train_acc=0.7835109058927001, val_acc=0.779491341991342\n",
            "epoch 18, train_loss=0.557424932718277, validation_loss=0.586780352122856, train_acc=0.7719723834652594, val_acc=0.7714420995670995\n",
            "epoch 19, train_loss=0.6132825613021851, validation_loss=0.6109749024564569, train_acc=0.7677627528583992, val_acc=0.7695481601731602\n",
            "test_loss=0.6517266062589792, test_acc=0.761574263502455\n",
            "test_AUC= 0.871628976611545\n",
            "pres= 1.0 rec= 0.02078179119247897 f1= 0.0407174018419777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7326/7326 [00:30<00:00, 241.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0.1107016107016107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(metrics), np.std(metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Wg1BSfmLuc",
        "outputId": "6fd431b4-d8e5-42ad-fb1e-7f7977ac0aa9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10215670215670214 0.016141820741678834\n"
          ]
        }
      ]
    }
  ]
}